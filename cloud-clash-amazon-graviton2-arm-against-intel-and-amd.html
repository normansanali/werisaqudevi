<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><title>CPU Chip Topologies - Amazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute - JoltDash</title><meta name=description content="Having explained that the various cloud instances can vary quite dramatically in their hardware configurations even though on paper they have the same delivered vCPU count, it would be interesting to dwell a little bit more into the CPU topologies and resulting aspects such as the core-to-core latencies. Frustrated with some inflexible or inaccurate public"><meta name=author content="Some Person"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"JoltDash","url":"\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Organization","name":"","url":"\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"\/","name":"home"}},{"@type":"ListItem","position":3,"item":{"@id":"\/cloud-clash-amazon-graviton2-arm-against-intel-and-amd.html","name":"CPU chip topologies amazon\u0027s arm based graviton2 against amd and intel comparing cloud compute"}}]}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","author":{"name":"Larita Shotwell"},"headline":"CPU Chip Topologies - Amazon\u0027s Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute","description":"Having explained that the various cloud instances can vary quite dramatically in their hardware configurations even though on paper they have the same delivered vCPU count, it would be interesting to dwell a little bit more into the CPU topologies and resulting aspects such as the core-to-core latencies. Frustrated with some inflexible or inaccurate public","inLanguage":"en","wordCount":1361,"datePublished":"2024-08-19T00:00:00","dateModified":"2024-08-19T00:00:00","image":"\/img\/avatar-icon.png","keywords":[""],"mainEntityOfPage":"\/cloud-clash-amazon-graviton2-arm-against-intel-and-amd.html","publisher":{"@type":"Organization","name":"\/","logo":{"@type":"ImageObject","url":"\/img\/avatar-icon.png","height":60,"width":60}}}</script><meta property="og:title" content="CPU Chip Topologies - Amazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute"><meta property="og:description" content="Having explained that the various cloud instances can vary quite dramatically in their hardware configurations even though on paper they have the same delivered vCPU count, it would be interesting to dwell a little bit more into the CPU topologies and resulting aspects such as the core-to-core latencies. Frustrated with some inflexible or inaccurate public"><meta property="og:image" content="/img/avatar-icon.png"><meta property="og:url" content="/cloud-clash-amazon-graviton2-arm-against-intel-and-amd.html"><meta property="og:type" content="website"><meta property="og:site_name" content="JoltDash"><meta name=twitter:title content="CPU Chip Topologies - Amazon's Arm-based Graviton2 Against AMD and …"><meta name=twitter:description content="Having explained that the various cloud instances can vary quite dramatically in their hardware configurations even though on paper they have the same delivered vCPU count, it would be interesting to …"><meta name=twitter:image content="/img/avatar-icon.png"><meta name=twitter:card content="summary"><meta name=twitter:site content="@username"><meta name=twitter:creator content="@username"><link href=./img/favicon.ico rel=icon type=image/x-icon><meta name=generator content="Hugo 0.98.0"><link rel=alternate href=./index.xml type=application/rss+xml title=JoltDash><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css integrity=sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.5.0/css/all.css integrity=sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/highlight.min.css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/codeblock.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css integrity=sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css integrity=sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R crossorigin=anonymous></head><body><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header class=header-section><div class="intro-header no-img"><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><h1>CPU Chip Topologies - Amazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute</h1><span class=post-meta><i class="fas fa-calendar"></i>&nbsp;Posted on August 19, 2024
&nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;7&nbsp;minutes
&nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1361&nbsp;words
&nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;Larita Shotwell</span></div></div></div></div></div></header><div class=container role=main><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><article role=main class=blog-post><h2>CPU Chip Topologies</h2><p>Having explained that the various cloud instances can vary quite dramatically in their hardware configurations even though on paper they have the same delivered “vCPU” count, it would be interesting to dwell a little bit more into the CPU topologies and resulting aspects such as the core-to-core latencies. Frustrated with some inflexible or inaccurate public tools on the matter, I recently had the time to write a new custom microbenchmark for testing synchronisation latencies of CPU cores, exhibiting some of the cache-coherency as well as physical layouts of current designs.</p><p>The following tables are core-to-core synchronisation latencies in nanoseconds.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/Graviton1-LDXSTX_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>Graviton1 - a1.4xlarge - Arm v8.0 ldaex/stlex</p><p>I thought that first it would be interesting to go back and showcase how Amazon’s first-generation Graviton SoC fared in this regard. Powered by Cortex-A72 cores, this design had its 16 cores arranged into 4 clusters, connected via coherent crossbar interconnect. Each cluster of 4x A72 cores had its own 2MB L2 cache, and we clearly see the faster access latencies within such a cluster in the above results. Coherency going from one cluster to the other incurred quite a high penalty, almost quadrupling the access latency.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/Graviton2-LDXSTX-affinity0_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>Graviton2 - m6g.16xlarge - Arm v8.0 ldaex/stlex<br>Cache line Near Core 0</p><p>Moving onto the Graviton2 with its 64 N1 cores, we see quite a different setup that is comparatively a lot more uniform. This makes sense as the chip’s cores are connected via a mesh network, and the chip is a single monolithic die design. We do see some odd results in the latencies though, most particularly with the lower numbered cores having seemingly better access latencies between each other than the higher numbered cores. I didn’t quite understand this behaviour as in theory the latencies should behave more evenly across the mesh.</p><p>Experimenting around, I saw that the results weren’t consistent across runs. Changing the CPU affinity around did have some larger impact on the results, until I understood what was happening.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/Graviton2-LDXSTX-affinity63_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>Graviton2 - m6g.16xlarge - Arm v8.0 ldaex/stlex<br>Cache line Near Core 63</p><p>In fact, what we’re seeing in the two above result sets isn’t the core-to-core two-point latency across the mesh, but rather the core-to-cache-to-core&nbsp;three-point latencies&nbsp;of the system. Amazon and Arm had confirmed one particular odd aspect of the CMN-600: cache lines are statically resident across the mesh’s cache slices. When allocating a cache line in the memory address space, this undergoes a particular hashing function which determines on which mesh cache slice it is physically homed in. When accessing this cache line from any CPU in the system, it always accesses the same physical L3 cache slice on the chip.</p><p>My particular test here consists of three primary threads: the main timing thread, and two ping-pong’ing bouncer threads which alter a flag on a cache-line allocated at the start of the test on the main thread. The behaviour which ensues&nbsp;in the above results table, is that when the flag cache line resides in one corner of the chip (Core pair 54 & 55 to be exact in this results set), and we have two cores on the opposite side of the chip accessing this cache line, it means the data must move forth across the whole chip and back even though it’s only being used by two cores adjacent to each other. Similarly, if two cores are synchronising on a cache line that is homed on a physically near cache slice in the mesh, the latencies will be significantly better as it has to travel much less distance.</p><p>It’s quite the odd behaviour that we don’t see quite as prevalent in other meshed cache interconnect systems such as Intel’s Xeons, although admittedly we don’t know if that’s just because we haven’t seen the same large core count implemented in such a manner, or if they have some sort of smarter cache line handling in such scenarios.</p><p>Amazon’s Graviton2 doesn’t partition the L3 mesh cache when launching lower vCPU count instances, and each instance in the system competitively shares the full 32MB cache with each other. I didn’t further spend time on this theory in my testing time, but it would be quite the odd result if you’re running some sort of high-synchronisation bottlenecked workload and the performance would vary just depending on where your shared cache line ends up on the chip relative to your active instance cores, food for thought for some database workloads.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/03_Infra%20Tech%20Day%202019_Filippo%20Neoverse%20N1%20FINAL%20WM22_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>The above results were core-to-core latencies implemented via Arm v8.0 exclusive load and stores, which isn’t really how you should do things on new chips such as the Neoverse N1 based Graviton2. As the N1 cores are v8.2 compatible, it means that they also implement the v8.1 ISA which add new instructions such as atomic compare-and-set (CAS).</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/Graviton2-CAS_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>Graviton2 - m6g.16xlarge - Arm v8.1&nbsp;CAS</p><p>Using CAS instructions for synchronisation is massively more efficient and faster than using several sequential operations for altering a value, as the change can be done in a single atomic operation, vastly reducing the back-and-forth coherency traffic across the chip. In absolute figures, this results in latencies almost 4 times faster than the equivalent Arm v8.0 mechanism.</p><p>What’s important here in terms of takeaways, is that if you are deploying software on the new Graviton2 system and upcoming other Neoverse N1 platforms, is that you should pay very close attention to make sure your software stack is compiled against Arm v8.1 or higher (N1 is v8.2).</p><p>Currently I don’t know how many pre-compiled packages in Linux distributions even account for such scenarios as I imagine they’re all targeting the most common v8.0 baseline. Again, database applications and other synchronization heavy workloads will be most affected and it’s recommended you compile these from source – it would be an interesting performance test for AWS users who do deploy such workloads (Taking suggestions on such a real-world test setup!).</p><p>Looking at the CAS results again, besides the lower latencies, we also see a higher-level latency pattern across the mesh, with distinct separation into 16-core groups in terms of the latencies.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/05_Infra%20Tech%20Day%202019_Defilippi%20System%20FINAL%20WM14_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Essentially what I think we’re seeing here is the separation of the chip’s mesh network into “super tiles”, essentially mesh quadrants on the chip. Depending on how the data is routed on the mesh, this appears as a pattern across the latencies between the cores. It is quite odd that these results aren’t as uniform as the v8.0 results; I don’t have any technical explanation for this behaviour.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/numa-g_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Finally, it’s again noteworthy that we’re talking about a 64-core system with a single uniform memory domain and on a single monolithic die. AMD’s Rome does provide the former, but doesn’t achieve the same uniform access latencies as the Graviton2.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/Intel-Cascade-64-vCPU_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>Xeon Platinum 8259 (Cascade Lake) - m5n.16xlarge - x86 CAS</p><p>Looking at the Intel Xeon based instances for comparison, we see relatively uniform access across 16 cores in the same socket, with worse latencies for the CPUs in the second socket. The Graviton2’s latencies here are better than Intel in the best case, but also slightly worse in the worst case (in the same socket).</p><p>It’s to be noted that these systems have the SMT logical cores enumerated not at N+1 as in Windows, but instead first listing the cores in physical order first, and then listing the secondary SMT logical cores, hence the mirrored latencies from core 32 onwards. Noteworthy is the low logical-to-logical core latencies of 9.3ns (~3.2GHz cores), due to the coherency between such siblings happening at the L1D cache level.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/numa-n_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Having two sockets, we’re seeing two NUMA nodes for the Xeon system, dividing up the physical memory into two virtual memory spaces and two core sets.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/AMD-Epyc-64-vCPU_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>AMD EPYC 7571 - m5a.16xlarge - x86 CAS</p><p>Finally, the AMD EPYC system we see cores within an CCX having excellent latencies between each other, however coherency between different CCXs is slow, and even slower when accessing other dies within the socket.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15578/numa-a_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>The four NUMA nodes means the memory is divided into four virtual memory spaces as well as CPU groups. Again, AMD’s newer EPYC2 Rome processors get rid of this limitation, but alas weren’t able to test on AWS at this moment in time.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH52gZZxZpykn6qxbq%2FLmqqhZZGirru7zWaeq5mmnsGwupFmmKulXZa0orXNrKtmoZ6psq15wKebZpmdmXxz</p><hr><section id=social-share><div class="list-inline footer-links"><div class=share-box aria-hidden=true><ul class=share><li><a href="//twitter.com/share?url=%2fcloud-clash-amazon-graviton2-arm-against-intel-and-amd.html&text=CPU%20Chip%20Topologies%20-%20Amazon%27s%20Arm-based%20Graviton2%20Against%20AMD%20and%20Intel%3a%20Comparing%20Cloud%20Compute&via=username" target=_blank title="Share on Twitter"><i class="fab fa-twitter"></i></a></li><li><a href="//www.facebook.com/sharer/sharer.php?u=%2fcloud-clash-amazon-graviton2-arm-against-intel-and-amd.html" target=_blank title="Share on Facebook"><i class="fab fa-facebook"></i></a></li><li><a href="//reddit.com/submit?url=%2fcloud-clash-amazon-graviton2-arm-against-intel-and-amd.html&title=CPU%20Chip%20Topologies%20-%20Amazon%27s%20Arm-based%20Graviton2%20Against%20AMD%20and%20Intel%3a%20Comparing%20Cloud%20Compute" target=_blank title="Share on Reddit"><i class="fab fa-reddit"></i></a></li><li><a href="//www.linkedin.com/shareArticle?url=%2fcloud-clash-amazon-graviton2-arm-against-intel-and-amd.html&title=CPU%20Chip%20Topologies%20-%20Amazon%27s%20Arm-based%20Graviton2%20Against%20AMD%20and%20Intel%3a%20Comparing%20Cloud%20Compute" target=_blank title="Share on LinkedIn"><i class="fab fa-linkedin"></i></a></li><li><a href="//www.stumbleupon.com/submit?url=%2fcloud-clash-amazon-graviton2-arm-against-intel-and-amd.html&title=CPU%20Chip%20Topologies%20-%20Amazon%27s%20Arm-based%20Graviton2%20Against%20AMD%20and%20Intel%3a%20Comparing%20Cloud%20Compute" target=_blank title="Share on StumbleUpon"><i class="fab fa-stumbleupon"></i></a></li><li><a href="//www.pinterest.com/pin/create/button/?url=%2fcloud-clash-amazon-graviton2-arm-against-intel-and-amd.html&description=CPU%20Chip%20Topologies%20-%20Amazon%27s%20Arm-based%20Graviton2%20Against%20AMD%20and%20Intel%3a%20Comparing%20Cloud%20Compute" target=_blank title="Share on Pinterest"><i class="fab fa-pinterest"></i></a></li></ul></div></div></section></article><ul class="pager blog-pager"><li class=previous><a href=./dustin-diamond-dies-at-44-saved-by-the-bell-cast-more-celebs-react.html data-toggle=tooltip data-placement=top title="Dustin Diamond Dies at 44: 'Saved by the Bell' Cast, More Celebs React">&larr; Previous Post</a></li><li class=next><a href=./are-jerar-encarnacion-and-edwin-encarnacion-related-know-about-the-baseball-players-parents-330157-html.html data-toggle=tooltip data-placement=top title="Are Jerar Encarnacin And Edwin Encarnacin Related? Know About The Baseball Players Parents ">Next Post &rarr;</a></li></ul></div></div></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center footer-links"><li><a href title=RSS><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="credits copyright text-muted">All rights reserved
&nbsp;&bull;&nbsp;&copy;
2024
&nbsp;&bull;&nbsp;
<a href=./>JoltDash</a></p><p class="credits theme-by text-muted"><a href=https://gohugo.io>Hugo v0.98.0</a> powered &nbsp;&bull;&nbsp; Theme <a href=https://github.com/halogenica/beautifulhugo>Beautiful Hugo</a> adapted from <a href=https://deanattali.com/beautiful-jekyll/>Beautiful Jekyll</a></p></div></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js integrity=sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js integrity=sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe crossorigin=anonymous></script>
<script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script>
<script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script>
<script src=https://assets.cdnweb.info/hugo/bh/js/main.js></script>
<script src=https://assets.cdnweb.info/hugo/bh/js/highlight.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><script>$(document).ready(function(){$("pre.chroma").css("padding","0")})</script><script>renderMathInElement(document.body)</script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js integrity=sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js integrity=sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q crossorigin=anonymous></script><script src=https://assets.cdnweb.info/hugo/bh/js/load-photoswipe.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>